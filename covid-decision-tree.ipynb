{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d760bc",
   "metadata": {},
   "source": [
    "# Coronavírus RS\n",
    "\n",
    "Os dados foram coletados para apresentar os principais dados epidemiológicos da COVID-19 no Rio Grande do Sul.\n",
    "\n",
    "São utilizados como fontes os dois sistemas de notificações oficiais do Ministério da Saúde no monitoramento da doença: o e-SUS Notifica e o Sistema de Informação da Vigilância Epidemiológica da Gripe (Sivep-Gripe).\n",
    "\n",
    "O e-SUS Notifica (que antes era chamado de e-SUS VE) é a ferramenta na qual são notificados os casos de síndrome gripal (SG) que não precisam de internação hospitalar. Casos de SG atendidos em Unidades Sentinelas são notificados no SIVEP-Gripe (Síndrome Gripal). Nos casos em que a pessoa apresenta quadro mais grave da infecção (Síndrome Respiratória Aguda Grave – SRAG) e é necessária a hospitalização, a notificação é feita no SIVEP-Gripe (SRAG Hospitalizado)..\n",
    "\n",
    "Ambos os sistemas são utilizados pelos serviços de saúde públicos e privados e pelas secretarias municipais de saúde para realização das notificações e monitoramento dos casos e seus contactantes (sintomas, exames realizados, resultados, evolução, entre outros).\n",
    "\n",
    "Os casos confirmados de COVID-19 nos sistemas são diariamente identificados pela Secretaria Estadual da Saúde (SES) e publicados no painel, sendo utilizados os seguintes critérios de confirmação, conforme protocolos vigentes: laboratorial, clínico-epidemiológico, clínico-imagem e clínico.\n",
    "\n",
    "#### Caso confirmado de COVID-19 por critério laboratorial:\n",
    "- Resultado detectável para SARS-CoV-2 realizado pelo método RT-PCR, que detecta em amostra de secreções das vias aéreas (nariz e garganta) o vírus SARS-CoV-2, causador da COVID-19.\n",
    "\n",
    "- Resultado reagente em teste de antígeno imunocromatográfico oude imunofluorescência que detecta o vírus SARS-CoV-2.\n",
    "\n",
    "- Resultado reagente em testes sorológicos (testes rápidos de anticorpos, eletroquimioluminescência, ensaio imunoenzimático, entre outros), que identificam anticorpos produzidos pelo organismo em resposta à infecção pelo SARS-CoV-2.\n",
    "\n",
    "#### Caso confirmado de COVID-19 por critério clínico-epidemiológico:\n",
    "Caso de SG ou SRAG, sem confirmação laboratorial, com histórico de contato próximo ou domiciliar, nos 14 dias anteriores ao aparecimento dos sinais e sintomas, com caso confirmado laboratorialmente para COVID-19.\n",
    "\n",
    "#### Caso confirmado de COVID-19 por critério clínico-imagem:\n",
    "Caso de SG ou SRAG ou óbito por SRAG que não foi possível confirmar por critério laboratorial e que apresente alterações tomográficas indicativas de infecção por SARS-CoV-2.\n",
    "\n",
    "#### Caso confirmado de COVID-19 por critério clínico:\n",
    "Caso de SG ou SRAG associado à anosmia (disfunção olfativa) ou ageusia (disfunção gustatória) aguda sem outra causa pregressa e que não foi possível encerrar por outro critério de confirmação.\n",
    "\n",
    "#### Casos em acompanhamento e recuperados:\n",
    "Os casos em acompanhamento e recuperados são estimados por meio de um cálculo composto que leva em consideração os casos notificados com confirmação de COVID-19.\n",
    "\n",
    "**RECUPERADOS:** inclui os casos leves e moderados de SG com início dos sintomas há mais de 14 dias, que não hospitalizaram e não evoluíram para óbito, e o número de pacientes com SRAG hospitalizados com registro de alta no SIVEP-Gripe.\n",
    "\n",
    "As colunas DATA DE EVOLUÇÃO e DATA DE EVOLUÇÃO ESTIMADA nos dados de exportação deste painel refletem as informações coletadas de acordo com os critérios acima. Na coluna DATA DE EVOLUÇÃO consta a informação registrada pelo estabelecimento de saúde nos sistemas oficiais, já a coluna DATA DE EVOLUÇÃO ESTIMADA segue os critérios pré-estabelecidos para os casos leves e moderados ou por falta de preenchimento dos registros com alta hospitalar.\n",
    "\n",
    "#### Óbito:\n",
    "Caso de Síndrome Respiratória Aguda Grave (SRAG) confirmado para Covid-19 cuja infecção pelo coronavírus foi associada ao óbito.\n",
    "\n",
    "#### Fontes:\n",
    "Guia de Vigilância Epidemiológica - https://www.saude.gov.br/images/af_gvs_coronavirus_6ago20_ajustes-finais-2.pdf\n",
    "Brasil - www.covid.saude.gov.br - Ministério da Saúde\n",
    "*População: Estudo de Estimativas populacionais por município, sexo e idade - 2000-2020. Disponível em https://datasus.saude.gov.br/populacao-residente/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6618007",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## Sobre os dados\n",
    "\n",
    "Os dados que iremos trabalhar foram tratados e balanceados para o objetivo que será fazer uso de modelagem para classificar qual a chance do paciente ter obito ou recuperação.\n",
    "\n",
    "Caso deseja analisa todos os dados sem tratamento, [clique aqui](https://ti.saude.rs.gov.br/covid19/download).\n",
    "\n",
    "\n",
    "### Dicionário de Dados\n",
    "\n",
    "|Features|Description|\n",
    "|:--------|:-----------|\n",
    "|SEXO\t|Sexo |\n",
    "|FAIXAETARIA |\tFaixa Etária |\n",
    "|EVOLUCAO\t  | Descrição da evolução |\n",
    "|FEBRE\t      | Sintomas de febre |\n",
    "|TOSSE\t      |  Sintomas de tosse |\n",
    "|GARGANTA\t|Sintomas de dor de garganta |\n",
    "|DISPNEIA\t|Sintomas de dor de dispnéia/falta de ar |\n",
    "|GESTANTE\t|Paciente é gestante |\n",
    "|RACA_COR\t|Raça/Cor |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767bff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atraves da biblioteca pandas vamos importar o dataset covid_aluno.csv que se encontra dentro da pasta data.\n",
    "# Vamos atribuir o dataset a uma variavel chamada df_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e98cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos usar o função head do pandas para exibir parte do nosso dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e5694",
   "metadata": {},
   "source": [
    "A coluna \"Unnamed: 0\" representa a linha do dados antes do tratamento/analise de dados. Entendemos que não é uma coluna importante e iremos remover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como descrito acima, vamos remover a coluna 'Unnamed: 0' do nosso dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0df433",
   "metadata": {},
   "source": [
    "Como os dados foram tratados, iremos certificar se não temos dados null...\n",
    "\n",
    "Será possivel observsar que algumas features irão aparecer valores \"NAO INFORMADO\", não iremos desconsiderar, pois o modelo irá trabalhar com paciente quando não contem a informação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ac252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com a coluna removida, qual a função do pandas que podemos visualizar se temos dados null\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de235863",
   "metadata": {},
   "source": [
    "## Visualização/Transformação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temos o conhecimento que o nosso modelo decision tree trabalhar com dados numericos\n",
    "# Nosso dataset contem dados numericos? Como podemos verificar utilizando uma função do pandas?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80167ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dfe028",
   "metadata": {},
   "source": [
    "## Pandas Profiling\n",
    "\n",
    "`pandas-profiling` gera relatórios de perfil de um DataFrame pandas. A função pandas `df.describe()` é útil, mas um pouco básica para análise exploratória de dados. `pandas-profiling` estende o DataFrame do pandas com `df.profile_report()`, que gera automaticamente um relatório univariado e multivariado padronizado para compreensão dos dados.\n",
    "\n",
    "Para cada coluna, as seguintes informações (sempre que relevantes para o tipo de coluna) são apresentadas em um relatório HTML interativo:\n",
    "\n",
    "- **Inferência de tipo:** detectar os tipos de colunas em um DataFrame\n",
    "- **Essentials:** tipo, valores exclusivos, indicação de valores ausentes\n",
    "- **Estatísticas quantílicas:** valor mínimo, Q1, mediana, Q3, máximo, intervalo, intervalo interquartil\n",
    "- **Estatística descritiva:** média, moda, desvio padrão, soma, desvio absoluto mediano, coeficiente de variação, curtose, assimetria\n",
    "- **Valores mais frequentes e extremos**\n",
    "- **Histogramas:** categóricos e numéricos\n",
    "- **Correlações:** avisos de alta correlação, com base em diferentes métricas de correlação (Spearman, Pearson, Kendall, V de Cramér, Phik)\n",
    "- **Valores ausentes:** por meio de contagens, matriz, mapa de calor e dendrogramas\n",
    "- **Linhas duplicadas:** lista das linhas duplicadas mais comuns\n",
    "- **Análise de texto:** categorias mais comuns (maiúsculas, minúsculas, separadores), scripts (latim, cirílico) e blocos (ASCII, cirílico)\n",
    "- **Análise de arquivos e imagens:** tamanhos de arquivos, datas de criação, dimensões, indicação de imagens truncadas e existência de metadados EXIF\n",
    "\n",
    "O relatório contém três seções adicionais:\n",
    "- **Visão geral:** detalhes principalmente globais sobre o conjunto de dados (número de registros, número de variáveis, falta geral e duplicatas, espaço de memória)\n",
    "- **Alertas:** uma lista abrangente e automática de possíveis problemas de qualidade de dados (alta correlação, assimetria, uniformidade, zeros, valores ausentes, valores constantes, entre outros)\n",
    "- **Reprodução:** detalhes técnicos sobre a análise (tempo, versão e configuração)\n",
    "\n",
    "O pacote pode ser usado via código, mas também diretamente como um utilitário CLI. O relatório interativo gerado pode ser consumido e compartilhado como HTML normal ou incorporado de forma interativa dentro dos Jupyter Notebooks.\n",
    "\n",
    "\n",
    "[Instalando o pandas-profiling](https://pandas-profiling.ydata.ai/docs/master/pages/getting_started/installation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c52d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agora que falamos um pouco sobre o pandas_profiling vamos instalar o pacote e criar uma primeira visualização\n",
    "\n",
    "Objetivo:\n",
    "- criar uma exportação em HTML dos dados dentro da pasta data/reports\n",
    "- gerar o seguintes nome para o arquivo \"covid-prediction.html\"\n",
    "- criar um titulo \"COVID - Prediction\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c02428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fizemos a analise do arquivo covid-prediction.html e temos a necessidade transformar dados para o nosso modelo.\n",
    "Para não correr um risco e trabalharmos com versionamento, vamos criar uma copia do df_raw para a variavel df\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352af405",
   "metadata": {},
   "source": [
    "## OrdinalEncoder\n",
    "\n",
    "O OrdinalEncoder() substitui as categorias por dígitos, começando de 0 a k-1, onde k é o número de categorias diferentes. Se você selecionar “arbitrary” no **encoding_method**, o codificador atribuirá números conforme os rótulos aparecem na variável (primeiro a chegar, primeiro a ser servido). Se você selecionar “ordenado”, o codificador atribuirá números seguindo a média do valor alvo para esse rótulo. Portanto, rótulos para os quais a média do alvo é maior receberão o número 0, e aqueles em que a média do alvo for menor receberão o número k-1. Dessa forma, criamos uma relação monotônica entre a variável codificada e o alvo.\n",
    "\n",
    "\n",
    "```encoder = OrdinalEncoder(encoding_method='ordered', variables=['pclass', 'cabin', 'embarked'])```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28010b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "df['SEXO'] = encoder.fit_transform(pd.DataFrame(df['SEXO']))\n",
    "df['FAIXAETARIA'] = encoder.fit_transform(pd.DataFrame(df['FAIXAETARIA']))\n",
    "df['EVOLUCAO'] = encoder.fit_transform(pd.DataFrame(df['EVOLUCAO']))\n",
    "df['FEBRE'] = encoder.fit_transform(pd.DataFrame(df['FEBRE']))\n",
    "df['TOSSE'] = encoder.fit_transform(pd.DataFrame(df['TOSSE']))\n",
    "df['GARGANTA'] = encoder.fit_transform(pd.DataFrame(df['GARGANTA']))\n",
    "df['DISPNEIA'] = encoder.fit_transform(pd.DataFrame(df['DISPNEIA']))\n",
    "df['GESTANTE'] = encoder.fit_transform(pd.DataFrame(df['GESTANTE']))\n",
    "df['RACA_COR'] = encoder.fit_transform(pd.DataFrame(df['RACA_COR']))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76922f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformamos os dados!\n",
    "Vamos ver como ficou analisando no pandas_profiling\n",
    "\n",
    "Objetivo:\n",
    "- criar uma exportação em HTML dos dados dentro da pasta data/reports\n",
    "- gerar o seguintes nome para o arquivo \"covid-prediction-encoder.html\"\n",
    "- criar um titulo \"COVID - Prediction (Encoder)\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4fc7a8",
   "metadata": {},
   "source": [
    "## Análise exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ops... está execução irá apresentar um erro! \n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "data = df_raw[df_raw['EVOLUCAO'] == 'RECUPERADO']['FAIXAETARIA'].value_counts().sort_index()\n",
    "sns.lineplot(data=data)\n",
    "\n",
    "data = df_raw[df_raw['EVOLUCAO'] == 'OBITO']['FAIXAETARIA'].value_counts().sort_index()\n",
    "sns.lineplot(data=data)\n",
    "\n",
    "plt.title('COVID - Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "data = df_raw[['FAIXAETARIA', 'EVOLUCAO']].sort_values(by=['FAIXAETARIA'], ascending=True)\n",
    "sns.countplot(x ='FAIXAETARIA', hue = \"EVOLUCAO\", data = data)\n",
    "\n",
    "plt.title('FAIXAETARIA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.countplot(x ='FEBRE', hue = \"EVOLUCAO\", data = df_raw);\n",
    "plt.title('FEBRE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275eac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.countplot(x ='TOSSE', hue = \"EVOLUCAO\", data = df_raw);\n",
    "plt.title('TOSSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17343b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.countplot(x ='GESTANTE', hue = \"EVOLUCAO\", data = df_raw);\n",
    "plt.title('GESTANTE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386950a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.countplot(x ='RACA_COR', hue = \"EVOLUCAO\", data = df_raw);\n",
    "plt.title('RACA_COR');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad742bf",
   "metadata": {},
   "source": [
    "Como foi possivel observar as colunas abaixo contém uma alta correlação com a nossa variavel target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09834685",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Features são variáveis individuais e independentes que medem uma propriedade ou característica da tarefa. Escolher recursos informativos, discriminativos e independentes é a primeira decisão importante na implementação de qualquer modelo. No ML clássico, é nossa responsabilidade criar e escolher alguns recursos úteis dos dados, enquanto no aprendizado profundo moderno, os recursos são aprendidos automaticamente com base no algoritmo subjacente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df402482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vamos separar os nossos dados!\n",
    "\n",
    "Objetivos:\n",
    "- Em uma variavel df_features iremos manter todas as colunas com excessão da EVOLUCAO que é a nossa target\n",
    "- Em uma variavel df_classes iremos manter somente a coluna EVOLUCAO\n",
    "\n",
    "Como podemos fazer?\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b900417",
   "metadata": {},
   "source": [
    "## Conjuntos de Holdout (Retenção)\n",
    "\n",
    "O método de retenção para treinar um modelo de aprendizado de máquina é o processo de dividir os dados em diferentes divisões e usar uma divisão para treinar o modelo e outras divisões para validar e testar os modelos.\n",
    "\n",
    "![Alt text](data/imgs/holdout.png)\n",
    "\n",
    "Quando todos os dados são usados ​​para treinar o modelo usando algoritmos diferentes, o problema de avaliar os modelos e selecionar o modelo mais ideal permanece. A principal tarefa é descobrir qual modelo de todos os modelos tem o menor erro de generalização. Em outras palavras, qual modelo faz uma previsão melhor em conjuntos de dados futuros ou não vistos do que todos os outros modelos. É aqui que surge a necessidade de ter algum mecanismo em que o modelo seja treinado em um conjunto de dados e testado em outro conjunto de dados.\n",
    "\n",
    "O método hold-out para avaliação do modelo representa o mecanismo de divisão do conjunto de dados em conjuntos de dados de treinamento e teste. O modelo é treinado no conjunto de treinamento e, em seguida, testado no conjunto de teste para obter o modelo mais ideal.\n",
    "\n",
    "Essa abordagem tem a vantagem de ser simples de implementar, mas pode ser sensível à forma como os dados são divididos em dois conjuntos. Se a divisão não for aleatória, os resultados podem ser tendenciosos. No geral, o método hold out para avaliação de modelos é um bom ponto de partida para treinar modelos de aprendizado de máquina, mas deve ser usado com cautela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agora que entendemos um pouco mais do processo de validação holdout, \n",
    "Vamos separar nosso dados df_features, df_classes nas seguintes variaveis X_train, X_test, y_train, y_test \n",
    "com os parametros test_size=0.3, random_state=15\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se a executação da proxima linha apresentar uma mensagem de erro, \n",
    "# possivelmente seja devido ausencia do pacote mlxtend\n",
    "\n",
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from mlxtend.plotting import plot_decision_regions, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def cal_accuracy(y_test, y_predict): \n",
    "    plot_confusion_matrix(confusion_matrix(y_test, y_predict))\n",
    " \n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predict)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    print (f\"\\nAccuracy    : {accuracy_score(y_test,y_predict)*100:0.3f}\")\n",
    "    print (f\"Accuracy ROC: {roc_auc*100:0.3f}\")\n",
    "    \n",
    "    print (\"\\n\")\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183deb7",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Todos nós usamos a técnica da Árvore de Decisão diariamente para planejar nossa vida, apenas não damos um nome sofisticado a esse processo de tomada de decisão.\n",
    "\n",
    "Árvores de Decisão, ou Decision Trees, são algoritmos de machine learning largamente utilizados, com uma estrutura de simples compreensão e que costumam apresentar bons resultados em suas previsões.\n",
    "\n",
    "Eles também são a base do funcionamento de outros poderosos algoritmos, como o Random Forest.\n",
    "\n",
    "As decision trees estão entre os primeiros algoritmos aprendidos por iniciantes no mundo do aprendizado de máquina.\n",
    "\n",
    "Antes de entrarmos nos detalhes deste algoritmo, vamos entender de maneira simplificada o seu funcionamento. Apesar do grande poder de previsão de uma árvore de decisão, conhecer o seu funcionamento básico é algo muito simples e fácil.\n",
    "\n",
    "Mesmo quem está começando na área já será capaz de obter este entendimento.\n",
    "\n",
    "### Estrutura de uma árvore de decisão\n",
    "\n",
    "Como o próprio nome sugere, neste algoritmo vários pontos de decisão serão criados. Estes pontos são os “nós” da árvore e em cada um deles o resultado da decisão será seguir por um caminho, ou por outro. Os caminhos existentes são os “ramos”.\n",
    "\n",
    "Esta é a estrutura básica de uma árvore de decisão. Os nós são responsáveis pelas conferências que irão indicar um ramo ou outro para sequência do fluxo.\n",
    "\n",
    "Detalhando ainda mais esta lógica, uma pergunta será feita e teremos duas opções de resposta: sim ou não. A opção “sim” levará a uma próxima pergunta, e a opção “não” a outra.\n",
    "\n",
    "Estas novas perguntas também terão como opções de resposta o sim e não, e desta forma toda a árvore será construída, partindo de um ponto comum, podendo existir várias opções de caminhos diferentes a serem percorridos na árvore, cada um levando a um resultado.\n",
    "\n",
    "\n",
    "Na imagem abaixo vemos uma árvore de decisão extremamente simples, apenas com dois nós e poucos ramos.\n",
    "\n",
    "\n",
    "![Alt text](data/imgs/decision-tree-praia.png)\n",
    "\n",
    "É claro que, em uma situação como esta, não será necessário um algoritmo que nos diga se iremos a praia ou não, mas a ideia é válida para este aprendizado. Entendendo como esta árvore foi criada, você entenderá a lógica de uma árvore de decisão.\n",
    "\n",
    "![Alt text](data/imgs/decision-tree-praia-ds.png)\n",
    "\n",
    "Esta é a tabela que levou à criação da árvore apresentada. Como podemos ver, todas as vezes em que a coluna “Sol?” é igual a “Não”, a pessoa não foi para praia.\n",
    "\n",
    "Esta coluna foi utilizada na primeira pergunta, ou no primeiro nó de nossa árvore, sendo que sempre que a informação na coluna “Sol?” for “Não”, nossa árvore de decisão responderá que a pessoa não foi para praia.\n",
    "\n",
    "Porém quando esta resposta for “Sim”, temos casos onde a pessoa foi para praia e outros em que ela não foi.\n",
    "\n",
    "Ou seja, é necessário fazermos mais uma pergunta para definirmos a resposta, levando assim a segunda pergunta, ou segundo nó de nossa árvore, que irá conferir a informação existente na coluna “Vento?”.\n",
    "\n",
    "É importante observamos que esta mesma tabela poderia levar a construção de uma árvore diferente. Se primeiramente conferirmos o valor da variável “Vento?” veremos que em todos os dias com vento não fomos para praia.\n",
    "\n",
    "Em dias sem vento, conferimos também se havia sol, para então definir a resposta final.\n",
    "\n",
    "![Alt text](data/imgs/decision-tree-praia-vento.png)\n",
    "\n",
    "Neste simples exemplo vemos que normalmente não existirá uma única árvore de decisão para um mesmo problema, sendo que com diferentes árvores poderemos chegar a um mesmo resultado.\n",
    "\n",
    "\n",
    "### Definindo os nós e ramos\n",
    "\n",
    "Assim como podemos ter mais de uma árvore para um mesmo problema, também podemos utilizar diferentes métodos de cálculo na criação de uma árvore de decisão.\n",
    "\n",
    "Estes métodos são os responsáveis pela definição da estrutura e resultado final da árvore, e tentam buscar a estrutura mais otimizada para o problema em questão.\n",
    "\n",
    "![](data/imgs/decision-tree-praia-sim-nao.png)\n",
    "\n",
    "Imagine que, em nosso exemÁrvore de decisão com um nóplo anterior, os dados fossem um pouco diferentes, sendo que em todos os dias de sol fomos para a praia, e em dias sem sol, não fomos.\n",
    "\n",
    "Nesta situação, a árvore não precisaria conferir se existe vento ou não, pois independente desta informação o resultado seria o mesmo. Ou seja, nossa variável target “Vou para praia?” seria totalmente explicada pela variação da variável preditora “Sol?”, resultando em uma árvore com apenas um nó.\n",
    "\n",
    "Os métodos utilizados pelos algoritmos irão buscar justamente estas variáveis dentre todas as preditoras, identificando aquelas que possuem maior relação com a variável target, e colocando-as no topo da árvore, em seus nós principais.\n",
    "\n",
    "### Entropia \n",
    "Através da entropia o algoritmo verifica como os dados estão distribuídos nas variáveis preditoras de acordo com a variação da variável target. Quanto maior a entropia, maior a desordem dos dados; e quanto menor, maior será a ordem destes dados, quando analisados pela ótica da variável target.\n",
    "\n",
    "Partindo da entropia, o algoritmo confere o ganho de informação de cada variável. Aquela que apresentar maior ganho de informação será a variável do primeiro nó da árvores.\n",
    "\n",
    "<!-- ![](data/imgs/Entropy_3.png) -->\n",
    "\n",
    "Podemos entender o ganho de informação como a medida de quão bem relacionados os dados da variável preditora estão com os dados da variável target (ou o quanto a variável target pode ser explicada a partir da variável preditora), sendo que a variável com melhor desempenho será a escolhida para iniciar a árvore.\n",
    "\n",
    "### Índice GINI\n",
    "Com o cálculo do índice GINI, assim como na Entropia, será verificada a distribuição dos dados nas variáveis preditoras de acordo com a variação da variável target, porém com um método diferente.\n",
    "\n",
    "A variável preditora com o menor índice Gini será a escolhida para o nó principal da árvore, pois um baixo valor do índice indica maior ordem na distribuição dos dados.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Cada novo nó inserido na árvore é um novo critério que deve ser atendido. Ou seja, as amostras são divididas em cada nó, sendo que uma parte segue por um ramo, e a outra por outro. Desta forma é evidente que cada um destes novos nós receberá menos amostras que o anterior, pois o total de amostras que estava no nó anterior foi dividido em dois grupos.\n",
    "\n",
    "Esta lógica poderá continuar até que não existam mais critérios capazes de dividir as amostras em diferentes grupos. Neste momento teremos uma árvore de decisão completa, capaz de identificar cada amostra com a condição exata que a define.\n",
    "\n",
    "Podemos utilizar Previsões com Overfitting na árvore como esta para conhecer os dados em questão e entender seu relacionamento, porém normalmente ao utilizar um modelo de machine learning para prever novos valores a abordagem será diferente, visando garantir a não ocorrência de overfitting.\n",
    "\n",
    "Um modelo com overfitting é aquele que aprendeu muitos detalhes dos dados históricos que foram utilizados para treinar o modelo.\n",
    "\n",
    "Na prática, o modelo acabou decorando as condições, de maneira que ao receber novos dados, que costumam possuir relacionamentos similares mas não iguais, estas pequenas diferenças farão o modelo não encontrar as condições exatas que ele decorou, e, por consequência, sua previsão estará errada.\n",
    "\n",
    "Em uma árvore de decisão, é comum definirmos um número mínimo de amostras para cada novo nó, ou ainda um número máximo de nós para cada ramo. Assim, mesmo que as amostras de um ramo sejam suficientes para criação de um novo nó, caso o número de amostras seja inferior ao que definimos, ou o número máximo de nós do ramo já tenha sido atingido, o nó não será criado, de modo que todas as amostras que estão neste ramo receberão o mesmo resultado.\n",
    "\n",
    "Com isso, talvez nossos erros aumentem, porém ao apresentar novos dados ao modelo, a probabilidade de uma previsão correta poderá ser maior.\n",
    "\n",
    "Estas alternativas visam a construção de modelos generalizáveis, que podem ser aplicados em diferentes bases de dados, mantendo bons resultados.\n",
    "\n",
    "<p style='text-align: right; font-size: 10px; color: #CCC'>Fonte: https://didatica.tech/como-funciona-o-algoritmo-arvore-de-decisao</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "model_decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
    "model_decision_tree.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e515049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "dot_data = tree.export_graphviz(model_decision_tree, \n",
    "                                feature_names = list(X_train), class_names = ['0', '1'], \n",
    "                                out_file = None,\n",
    "                                rounded=True,\n",
    "                                filled=True\n",
    "                               ) \n",
    "Source(dot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model_decision_tree.predict(X_test)\n",
    "\n",
    "cal_accuracy(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ce61d",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "RandomForest é um algoritmo de aprendizado supervisionado. Pode ser usado tanto para classificação quanto para regressão. É também o algoritmo mais flexível e fácil de usar. A floresta é composta de árvores. Diz-se que quanto mais árvores tem, mais robusta é a floresta. RandomForest criam árvores de decisão em amostras de dados selecionadas aleatoriamente, obtém previsões de cada árvore e seleciona a melhor solução por meio de votação. Ele também fornece um bom indicador da \"feature importance.\".\n",
    "\n",
    "RandomForest têm uma variedade de aplicações, como mecanismos de recomendação, classificação de imagens e seleção de recursos. Ele pode ser usado para classificar candidatos a empréstimos, identificar atividades fraudulentas e prever doenças.\n",
    "\n",
    "### Como funciona o algoritmo?\n",
    "\n",
    "Algumas etapas:\n",
    "- Selecione amostras aleatórias do conjunto de dados.\n",
    "- Constrói uma árvore de decisão para cada amostra e obtem um resultado de previsão de cada árvore de decisão.\n",
    "- Realize uma votação para cada resultado previsto.\n",
    "- Selecione o resultado da previsão com mais votos como a previsão final.\n",
    "\n",
    "![Alt text](data/imgs/voting_dnjweq.webp)\n",
    "\n",
    "\n",
    "**Vantagens:**\n",
    "- Random forests são consideradas um método altamente preciso e robusto devido ao número de árvores de decisão que participam do processo.\n",
    "- Não sofre do problema de overfitting. A principal razão é que leva a média de todas as previsões, o que cancela os vies.\n",
    "- O algoritmo pode ser usado em problemas de classificação e regressão.\n",
    "- Random forests também podem lidar com valores ausentes. Há duas maneiras de lidar com isso: usando valores medianos para substituir variáveis contínuas e calculando a média ponderada de proximidade de valores ausentes.\n",
    "- Podemos obter a importância relativa do features, o que ajuda a selecionar os recursos que mais contribuem para o classificador.\n",
    "\n",
    "**Desvantagens:**\n",
    "- Random forests são lentas na geração de previsões porque possuem várias árvores de decisão. Sempre que faz uma previsão, todas as árvores na floresta precisam fazer uma previsão para a mesma entrada fornecida e, em seguida, realizar a votação nela. Todo esse processo é demorado.\n",
    "- O modelo é difícil de interpretar em comparação com uma árvore de decisão, onde você pode facilmente tomar uma decisão seguindo o caminho na árvore.\n",
    "\n",
    "![Alt text](data/imgs/random_forests.png)\n",
    "\n",
    "### Random Forests vs Decision Trees\n",
    "\n",
    "- Random forests são um conjunto de múltiplas árvores de decisão.\n",
    "- Decision trees profundas podem sofrer de overfitting, mas Random forests evitam overfitting criando árvores em subconjuntos aleatórios.\n",
    "- Decision trees são computacionalmente mais rápidas.\n",
    "- Random forests são difíceis de interpretar, enquanto uma Decision trees é facilmente interpretável e pode ser convertida em regras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd60ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc=RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "rfc.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec52c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = rfc.predict(X_test)\n",
    "cal_accuracy(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0853f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    \"actual\": y_test, \n",
    "    \"predicted\": rfc.predict((X_test)),\n",
    "    \"% to 0\": rfc.predict_proba((X_test))[:,0],\n",
    "    \"% to 1\": rfc.predict_proba((X_test))[:,1],\n",
    "}) \n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486af9b7",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "**feature_importances_** é uma pontuação atribuída aos recursos de um modelo  que define o quão “importante” é um recurso para o modelo. Isso pode ajudar na seleção de recursos e podemos obter informações muito úteis sobre os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd37d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_evaluation import plot\n",
    "\n",
    "feat_importances = pd.Series(rfc.feature_importances_, index=X_train.columns)\n",
    "feat_importances.nlargest(30).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786af612",
   "metadata": {},
   "source": [
    "Quanto mais alto, mais importante o recurso. A importância de uma característica é computada como a redução total (normalizada) do critério trazido por aquela característica.\n",
    "\n",
    "Usando o algoritmo RandomForestClassifier, a importância do recurso pode ser medida como a diminuição média de impureza calculada a partir de todas as árvores de decisão na floresta. Isso independentemente do fato de os dados serem lineares ou não lineares (linearmente inseparáveis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834fd34",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb10e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models=pd.DataFrame({'Model':['Decision Tree','Random Forest'],\n",
    "                     'Test Accuracy':[model_decision_tree.score(X_test,y_test),rfc.score(X_test,y_test)]})\n",
    "\n",
    "df_models.sort_values(by='Test Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e883ab",
   "metadata": {},
   "source": [
    "## Parâmetros vs Hiperparâmetros\n",
    "\n",
    "Parâmetros e hiperparâmetros estão associados ao modelo de Machine Learning, mas ambos são destinados a tarefas diferentes. Vamos entender como eles são diferentes entre si no contexto do Machine Learning.\n",
    "\n",
    "Os **parâmetros** são as variáveis usadas pelo algoritmo de Machine Learning para prever os resultados com base nos dados históricos de entrada. Estes são estimados usando um algoritmo de otimização pelo próprio algoritmo de Machine Learning. Assim, essas variáveis não são definidas ou codificadas pelo usuário ou profissional. Essas variáveis são servidas como parte do treinamento do modelo. \n",
    "\n",
    "\n",
    "Os **hiperparâmetros** são as variáveis que o usuário normalmente especifica ao construir o modelo de Machine Learning. assim, os hiperparâmetros são especificados antes de especificar os parâmetros ou podemos dizer que os hiperparâmetros são usados para avaliar os parâmetros ótimos do modelo. a melhor parte dos hiperparâmetros é que seus valores são decididos pelo usuário que está construindo o modelo. Por exemplo, max_depth em algoritmos de Random Forest, k em KNN Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': [2**i for i in range(3, 10)],\n",
    "    'max_depth': [2, 4, 8, 16, 32, None]\n",
    "}\n",
    "\n",
    "pprint(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438f121",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Agora que sabemos o que são hiperparâmetros, nosso objetivo deve ser encontrar os melhores valores de hiperparâmetros para obter os resultados de previsão perfeitos do nosso modelo. Mas surge a pergunta: como encontrar esses melhores conjuntos de hiperparâmetros?\n",
    "\n",
    "Por esta razão, métodos como GridSearch foram introduzidos.\n",
    "\n",
    "O Grid Search usa uma combinação diferente de todos os hiperparâmetros especificados e seus valores e calcula o desempenho de cada combinação e seleciona o melhor valor para os hiperparâmetros. Isso torna o processamento demorado e caro com base no número de hiperparâmetros envolvidos.\n",
    "\n",
    "\n",
    "### Cross-Validation e GridSearchCV\n",
    "\n",
    "No GridSearchCV, juntamente com o Grid Search, a validação cruzada também é realizada. A validação cruzada é usada durante o treinamento do modelo. Como sabemos que antes de treinar o modelo com dados, dividimos os dados em duas partes – dados de treinamento e dados de teste. Na validação cruzada, o processo divide os dados do trem ainda mais em duas partes – os dados do trem e os dados de validação.\n",
    "\n",
    "O tipo mais popular de validação cruzada é a validação cruzada K-fold. É um processo iterativo que divide os dados do trem em k partições. Cada iteração mantém uma partição para teste e as k-1 partições restantes para treinamento do modelo. A próxima iteração definirá a próxima partição como dados de teste e os k-1 restantes como dados de trem e assim por diante. Em cada iteração, ele registrará o desempenho do modelo e no final fornecerá a média de todo o desempenho. Assim, também é um processo demorado.\n",
    "\n",
    "Assim, o GridSearch junto com a validação cruzada leva muito tempo cumulativamente para avaliar os melhores hiperparâmetros.\n",
    "\n",
    "![Alt text](data/imgs/K-fold_cross_validation_EN.svg)\n",
    "\n",
    "\n",
    "Principalmente, são necessários 4 argumentos, ou seja, estimator, param_grid, cv, e scoring.\n",
    "1. **estimator** - Um modelo scikit-learn\n",
    "2. **param_grid** – Um dicionário com nomes de parâmetros como chaves e listas de valores de parâmetros.\n",
    "3. **scoring** – A medida de desempenho. Por exemplo, 'r2' para modelos de regressão, 'accuracy' para modelos de classificação.\n",
    "4. **cv** – Um número inteiro que é o número de dobras para validação cruzada K-fold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "clf = GridSearchCV(rfc, parameters, cv=2, scoring='accuracy')\n",
    "clf.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d49123",
   "metadata": {},
   "source": [
    "Melhores Params e Melhor Pontuação do Random Forest Classifier\n",
    "\n",
    "Assim, **clf.best_params_** fornece a melhor combinação de hiperparâmetros ajustados e **clf.best_score_** fornece a pontuação média de validação cruzada de nosso Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Melhor Parâmetros: {}'.format(clf.best_params_))\n",
    "print('Melhor Score     : {}'.format(clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ff7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    \n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))\n",
    "        \n",
    "print_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b87e0",
   "metadata": {},
   "source": [
    "## Melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "\n",
    "best_model = clf.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "bestmodel_train = best_model.predict(X_train) #remover\n",
    "bestmodel_test = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('Testing  F1 Score        :', f1_score(bestmodel_test, y_test))\n",
    "print('Testing  Accuracy Score  :', accuracy_score(bestmodel_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_accuracy(y_test, bestmodel_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59fbd8",
   "metadata": {},
   "source": [
    "## ROC\n",
    "\n",
    "Para avaliar o quão bem um modelo se ajusta a um conjunto de dados, podemos observar as duas métricas a seguir:\n",
    "\n",
    "- **Sensibilidade**: A probabilidade de que o modelo preve um resultado positivo para uma observação quando, de fato, o resultado é positivo. \"true positive rate”.\n",
    "- **Especificidade**: A probabilidade de que o modelo preve um resultado negativo para uma observação quando, de fato, o resultado é negativo. “true negative rate”.\n",
    "\n",
    "Uma maneira de visualizar essas duas métricas é criando uma curva ROC, que significa curva de “característica de operação do receptor”. \n",
    "\n",
    "Este é um gráfico que mostra a sensibilidade e especificidade de um modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fcaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_score = best_model.predict_proba(X_test)[:,1]\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_score)\n",
    "\n",
    "plt.subplots(1, figsize=(10,6))\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "\n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive')\n",
    "plt.xlabel('False Positive')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4591af80",
   "metadata": {},
   "source": [
    "As curvas ROC normalmente apresentam taxa de verdadeiro positivo no eixo Y e taxa de falso positivo no eixo X. Isso significa que o canto superior esquerdo do gráfico é o ponto “ideal” - uma taxa de falsos positivos de zero e uma taxa de verdadeiros positivos de um. Isso não é muito realista, mas significa que uma área maior sob a curva (AUC) geralmente é melhor.\n",
    "\n",
    "A “inclinação” das curvas ROC também é importante, pois é ideal maximizar a taxa de verdadeiros positivos e minimizar a taxa de falsos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(best_model, open('data/models/rfc_prediction_covid_evolucao.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
